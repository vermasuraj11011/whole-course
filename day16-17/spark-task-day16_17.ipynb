{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a2fdd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the spark application\n"
     ]
    }
   ],
   "source": [
    "println(\"Welcome to the spark application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d5cad",
   "metadata": {},
   "source": [
    "# Case Study 1: Genre-Specific Data Aggregation Pipeline\n",
    "\n",
    "Objective: Aggregate movie ratings by genre and store the results in a Parquet format for analytics.\n",
    "\n",
    "Scenario: The Movielens dataset is stored in GCP Cloud Storage as CSV files. You need to calculate the average ratings per genre for analytics. Some genre information requires custom transformations due to inconsistent formats.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Ingestion: Load the movies.csv and ratings.csv files as DataFrames from GCP Cloud Storage.\n",
    "\n",
    "movies.csv contains columns: movieId, title, genres. ratings.csv contains columns: userId, movieId, rating, timestamp.\n",
    "\n",
    "Transformation: Use DataFrames to parse and explode the genres column into individual genre rows (e.g., split Action|Comedy into two rows: Action and Comedy).\n",
    "Convert to an RDD for custom transformations to handle inconsistent genre names (e.g., mapping Sci-Fi to Science Fiction).\n",
    "\n",
    "\n",
    "Aggregation: Perform the join between movies and ratings on movieId using a DataFrame.\n",
    "Use RDD transformations to calculate the average rating for each genre using a combination of reduceByKey and custom key-value mapping.\n",
    "\n",
    "Storage: Convert the RDD back to a DataFrame and save the aggregated results in Parquet format in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f49398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program ended\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "basePath = gs://scala-spark-temp/movies/\n",
       "movieFile = movie.csv\n",
       "ratingFile = rating.csv\n",
       "movieDf = [movieId: int, title: string ... 1 more field]\n",
       "ratingDf = [userId: int, movieId: int ... 2 more fields]\n",
       "moviesWithGenresDf = [movieId: int, title: string ... 1 more field]\n",
       "ratingWithMovies = [userId: int, movieId: int ... 4 more fields]\n",
       "ratingWithMoviesRdd = MapPartitionsRDD[109] at rdd at <console>:60\n",
       "transformedRdd = MapPartitionsRDD[110] a...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[110] a..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{explode, split, col}\n",
    "\n",
    "val basePath   = \"gs://scala-spark-temp/movies/\"\n",
    "val movieFile  = \"movie.csv\"\n",
    "val ratingFile = \"rating.csv\"\n",
    "\n",
    "val movieDf  = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(basePath + movieFile)\n",
    "val ratingDf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(basePath + ratingFile)\n",
    "val moviesWithGenresDf =\n",
    "movieDf.withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\"))).select(\"movieId\", \"title\", \"genre\")\n",
    "\n",
    "val ratingWithMovies =\n",
    "  ratingDf\n",
    "    .join(moviesWithGenresDf, ratingDf(\"movieId\") === moviesWithGenresDf(\"movieId\"), \"inner\")\n",
    "    .select(\n",
    "      ratingDf(\"userId\"),\n",
    "      ratingDf(\"movieId\"),\n",
    "      ratingDf(\"rating\"),\n",
    "      ratingDf(\"timestamp\"),\n",
    "      moviesWithGenresDf(\"title\"),\n",
    "      moviesWithGenresDf(\"genre\")\n",
    "    )\n",
    "\n",
    "val ratingWithMoviesRdd = ratingWithMovies.rdd\n",
    "\n",
    "val transformedRdd =\n",
    "      ratingWithMoviesRdd.map { row =>\n",
    "        val genre =\n",
    "          row.getAs[String](\"genre\") match {\n",
    "            case \"Sci-Fi\" =>\n",
    "              \"Science Fiction\"\n",
    "            case other =>\n",
    "              other\n",
    "          }\n",
    "        (genre, (row.getAs[Double](\"rating\"), 1))\n",
    "      }\n",
    "\n",
    "val reducedRdd =\n",
    "      transformedRdd\n",
    "        .reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    "        .mapValues { case (sum, count) =>\n",
    "          sum / count\n",
    "        }\n",
    "\n",
    "val genreRatingsDf = spark.createDataFrame(reducedRdd).toDF(\"genre\", \"average_rating\")\n",
    "\n",
    "genreRatingsDf.write.mode(\"overwrite\").parquet(\"hdfs://10.128.0.5:8020/user/suraj/rating\")\n",
    "\n",
    "// ratingWithMovies.write.mode(\"overwrite\").parquet(\"gs://scala-spark-temp/movies/output\")\n",
    "\n",
    "// ratingWithMovies.write.mode(\"overwrite\").parquet(\"hdfs://10.128.0.5:8020/user/suraj/output\")\n",
    "println(\"program ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa313e3a",
   "metadata": {},
   "source": [
    "# Case Study 2: User Rating History Partitioning\n",
    "\n",
    "Objective: Partition the Movielens dataset by user for faster query processing.\n",
    "\n",
    "Scenario: Movielens user ratings data (CSV format) needs to be partitioned into separate folders for each user in HDFS.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Ingestion: Load the ratings.csv file as a DataFrame from GCP Cloud Storage.\n",
    "\n",
    "Transformation: Use a DataFrame to filter out invalid or incomplete records.\n",
    "Convert the DataFrame into an RDD to dynamically create key-value pairs of userId and their corresponding ratings.\n",
    "\n",
    "Partitioning: Use RDD transformations like groupByKey to partition ratings data by userId.\n",
    "Write each user's data to a separate folder in HDFS using the saveAsTextFile method.\n",
    "\n",
    "Verification: Validate that the HDFS structure follows the format /user-data/{userId}/ratings.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38aeaab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application started .....\n",
      "read the data\n",
      "program ended .....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "basePath = gs://scala-spark-temp/movies/\n",
       "ratingFile = rating.csv\n",
       "ratingDf = [userId: int, movieId: int ... 2 more fields]\n",
       "validRatingDf = [userId: int, movieId: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 2 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"application started .....\")\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{explode, split, col}\n",
    "\n",
    "val basePath   = \"gs://scala-spark-temp/movies/\"\n",
    "val ratingFile = \"rating.csv\"\n",
    "val hdfsPath  = \"hdfs://10.128.0.5:8020/user/suraj/movies/\"\n",
    "\n",
    "val ratingDf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(basePath + ratingFile)\n",
    "\n",
    "println(\"read the data\")\n",
    "\n",
    "\n",
    "val validRatingDf =\n",
    "    ratingDf.filter(\n",
    "        col(\"userId\").isNotNull && col(\"movieId\").isNotNull && col(\"rating\").isNotNull && col(\"timestamp\").isNotNull\n",
    "    )\n",
    "\n",
    "// val ratingRdd = validRatingDf.rdd.map(row => (row.getAs[Int](\"userId\"), row.mkString(\",\")))\n",
    "\n",
    "// val partitionedRdd = ratingRdd.groupByKey()\n",
    "\n",
    "validRatingDf.write.partitionBy(\"userId\").mode(\"overwrite\").parquet(hdfsPath + \"ratings.parquet\")\n",
    "\n",
    "// partitionedRdd.collect().foreach { case (userId, ratings) =>\n",
    "//     try {\n",
    "//         val userRatingsRdd = spark.sparkContext.parallelize(ratings.toSeq)\n",
    "//         val outputPath = s\"hdfs://10.128.0.5:8020/user/suraj/user-data/$userId/ratings\"\n",
    "//         userRatingsRdd.saveAsTextFile(outputPath)\n",
    "//     } catch {\n",
    "//         case e: Exception =>\n",
    "//           println(s\"Error processing user $userId: ${e.getMessage}\")\n",
    "//     }\n",
    "// }\n",
    "\n",
    "println(\"program ended .....\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Alt text](task_2.png)",
   "id": "bac9339e7138bba0"
  },
  {
   "cell_type": "markdown",
   "id": "162c1ff6",
   "metadata": {},
   "source": [
    "# Case Study 3: Handling Incomplete Metadata\n",
    "Objective: Enrich incomplete movie metadata using additional JSON files.\n",
    "\n",
    "Scenario: Movielens metadata (e.g., movies.csv) is missing releaseYear for some movies. Supplementary metadata in JSON format is available for enrichment.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Ingestion: Load movies.csv from GCP Cloud Storage as a DataFrame.\n",
    "Load metadata.json from GCP Cloud Storage into an RDD for custom parsing.\n",
    "\n",
    "Transformation: Use RDD operations to parse the JSON file and extract movieId and releaseYear.\n",
    "Perform an RDD join with the movies DataFrame to fill in missing releaseYear.\n",
    "\n",
    "Validation: Convert the enriched RDD back into a DataFrame.\n",
    "Validate that all movies have a releaseYear field.\n",
    "\n",
    "Storage: Save the enriched DataFrame in Parquet format in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2460e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application started ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------+------------------+\n",
      "|movieId|title                               |genres            |\n",
      "+-------+------------------------------------+------------------+\n",
      "|125571 |The Court-Martial of Jackie Robinson|(no genres listed)|\n",
      "+-------+------------------------------------+------------------+\n",
      "\n",
      "+-------+-------------------------------------------+------------------+\n",
      "|movieId|title                                      |genres            |\n",
      "+-------+-------------------------------------------+------------------+\n",
      "|125571 |The Court-Martial of Jackie Robinson (1964)|(no genres listed)|\n",
      "+-------+-------------------------------------------+------------------+\n",
      "\n",
      "clossing application\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "basePath = gs://scala-spark-temp/movies/\n",
       "movieFile = movie.csv\n",
       "movieDf = [movieId: int, title: string ... 1 more field]\n",
       "metadataParsedDf = [movieId: bigint, release_date: bigint]\n",
       "updatedMovieDf = [movieId: int, title: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Row, SparkSession, functions=>F}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[movieId: int, title: string ... 1 more field]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Row, SparkSession, functions => F}\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n",
    "\n",
    "println(\"application started ...\")\n",
    "\n",
    "val basePath   = \"gs://scala-spark-temp/movies/\"\n",
    "val movieFile  = \"movie.csv\"\n",
    "\n",
    "val movieDf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(basePath + movieFile)\n",
    "val metadataParsedDf =\n",
    "      spark.read.option(\"inferSchema\", \"true\").json(basePath + \"movie_date.json\").drop(\"title\").drop(\"genres\")\n",
    "\n",
    "val updatedMovieDf =\n",
    "      movieDf\n",
    "        .join(metadataParsedDf, Seq(\"movieId\"), \"left\")\n",
    "        .withColumn(\n",
    "          \"title\",\n",
    "          F.when(F.col(\"title\").rlike(\"\\\\(\\\\d{4}\\\\)\"), F.col(\"title\"))\n",
    "            .otherwise(F.concat(F.col(\"title\"), F.lit(\" (\"), F.col(\"release_date\"), F.lit(\")\")))\n",
    "        )\n",
    "        .drop(\"release_date\")\n",
    "\n",
    "\n",
    "updatedMovieDf.write.mode(\"overwrite\").parquet(\"hdfs://10.128.0.5:8020/user/suraj/updatedMovies\")\n",
    "\n",
    "movieDf.filter(col(\"movieId\") === 125571).show(truncate = false)\n",
    "    \n",
    "updatedMovieDf.filter(col(\"movieId\") === 125571).show(truncate = false)\n",
    "\n",
    "println(\"clossing application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47cd94",
   "metadata": {},
   "source": [
    "# Case Study 4: Duplicate Record Removal Pipeline\n",
    "\n",
    "Objective: Identify and remove duplicate movie records based on movieId and title, saving clean data in Avro format.\n",
    "\n",
    "Scenario: The movies.csv file in HDFS contains duplicate records that need to be cleaned.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Ingestion: Load movies.csv into a Spark DataFrame from HDFS.\n",
    "\n",
    "Transformation:Use DataFrames to identify duplicates based on movieId and title.\n",
    "Convert the DataFrame to an RDD to perform custom filtering operations using distinct() on a composite key (movieId, title).\n",
    "\n",
    "Validation: Count the number of duplicates removed by comparing the record counts before and after transformation.\n",
    "\n",
    "Storage: Save the cleaned data as Avro files in GCP Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f27d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting application ...\n",
      "writing movies to hdfs\n",
      "reading from the hdfs file\n",
      "creating rdd from the dataframe\n",
      "clean dataframe from the distinct rdd\n",
      "Number of duplicates removed: 10\n",
      "before removing duplicates\n",
      "+-------+--------------------+-----+\n",
      "|movieId|               title|count|\n",
      "+-------+--------------------+-----+\n",
      "|   1267|Manchurian Candid...|    2|\n",
      "|   1807|Cool, Dry Place, ...|    2|\n",
      "|   2045|Far Off Place, A ...|    2|\n",
      "|    441|Dazed and Confuse...|    2|\n",
      "|      1|    Toy Story (1995)|    2|\n",
      "|     19|Ace Ventura: When...|    2|\n",
      "|    175|         Kids (1995)|    2|\n",
      "|   1489|Cats Don't Dance ...|    2|\n",
      "|    765|         Jack (1996)|    2|\n",
      "|   1014|    Pollyanna (1960)|    2|\n",
      "+-------+--------------------+-----+\n",
      "\n",
      "after removing duplicates\n",
      "+-------+-----+-----+\n",
      "|movieId|title|count|\n",
      "+-------+-----+-----+\n",
      "+-------+-----+-----+\n",
      "\n",
      "writing to gcp in avro formate\n",
      "clossing application ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n",
       "hdfsPath = hdfs://10.128.0.5:8020/user/suraj/movies/\n",
       "gcpPath = gs://scala-spark-temp/movies/\n",
       "fileName = movie_duplicate_record.csv\n",
       "gcpMovieDf = [movieId: int, title: string ... 1 more field]\n",
       "movieDf = [movieId: int, title: string ... 1 more field]\n",
       "movieRdd = MapPartitionsRDD[120] at map at <console>:41\n",
       "distinctMovieRdd = MapPartitionsRDD[124] at map at <console>:42\n",
       "cleanedMovieDf = [movieId: int, title: string ... 1 more field]\n",
       "originalCount = 27288\n",
       "cleanedCount = 27278\n",
       "duplicatesRemoved = 1...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "println(\"starting application ...\")\n",
    "\n",
    "val hdfsPath  = \"hdfs://10.128.0.5:8020/user/suraj/movies/\"\n",
    "val gcpPath = \"gs://scala-spark-temp/movies/\"\n",
    "val fileName = \"movie_duplicate_record.csv\"\n",
    "\n",
    "val gcpMovieDf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(gcpPath + fileName)\n",
    "\n",
    "println(\"writing movies to hdfs\")\n",
    "gcpMovieDf.write.option(\"header\", \"true\").mode(\"overwrite\").csv(hdfsPath + fileName)\n",
    "\n",
    "println(\"reading from the hdfs file\")\n",
    "val movieDf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(hdfsPath + fileName)\n",
    "\n",
    "println(\"creating rdd from the dataframe\")\n",
    "val movieRdd         = movieDf.rdd.map(row => ((row.getAs[Int](\"movieId\"), row.getAs[String](\"title\")), row))\n",
    "val distinctMovieRdd = movieRdd.distinct().map(_._2)\n",
    "\n",
    "println(\"clean dataframe from the distinct rdd\")\n",
    "val cleanedMovieDf = spark.createDataFrame(distinctMovieRdd, movieDf.schema)\n",
    "\n",
    "val originalCount     = movieDf.count()\n",
    "val cleanedCount      = cleanedMovieDf.count()\n",
    "val duplicatesRemoved = originalCount - cleanedCount\n",
    "println(s\"Number of duplicates removed: $duplicatesRemoved\")\n",
    "\n",
    "println(\"before removing duplicates\")\n",
    "val duplicateMovieDf = movieDf.groupBy(\"movieId\", \"title\").count().filter(col(\"count\") > 1)\n",
    "duplicateMovieDf.show()\n",
    "\n",
    "println(\"after removing duplicates\")\n",
    "cleanedMovieDf.groupBy(\"movieId\", \"title\").count().filter(col(\"count\") > 1).show()\n",
    "\n",
    "println(\"writing to gcp in avro formate\")\n",
    "cleanedMovieDf.write.format(\"avro\").mode(\"overwrite\").save(gcpPath + fileName)\n",
    "\n",
    "println(\"clossing application ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515816c",
   "metadata": {},
   "source": [
    "# Case Study 5: Time-Based Data Partitioning for Ratings\n",
    "Objective: Partition user ratings data by year and save in Parquet format.\n",
    "\n",
    "Scenario: The ratings.csv file includes a timestamp field that needs to be converted into human-readable years, and the data needs to be stored year-wise.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Ingestion: Load ratings.csv as a DataFrame from GCP Cloud Storage.\n",
    "\n",
    "Transformation:Use DataFrames to convert the timestamp field into a year column.\n",
    "Convert the DataFrame to an RDD to partition records by year using a key-value pair transformation.\n",
    "\n",
    "Partitioning: Save RDD partitions as separate Parquet files in HDFS, with the structure /ratings/{year}/ratings.parquet.\n",
    "\n",
    "Verification: Ensure that each year folder in HDFS contains only the records for that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23767ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting applications ...\n",
      "reading the rating csv from gcp\n",
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      2|   3.5|2005-04-02 23:53:47|\n",
      "|     1|     29|   3.5|2005-04-02 23:31:16|\n",
      "|     1|     32|   3.5|2005-04-02 23:33:39|\n",
      "|     1|     47|   3.5|2005-04-02 23:32:07|\n",
      "|     1|     50|   3.5|2005-04-02 23:29:40|\n",
      "|     1|    112|   3.5|2004-09-10 03:09:00|\n",
      "|     1|    151|   4.0|2004-09-10 03:08:54|\n",
      "|     1|    223|   4.0|2005-04-02 23:46:13|\n",
      "|     1|    253|   4.0|2005-04-02 23:35:40|\n",
      "|     1|    260|   4.0|2005-04-02 23:33:46|\n",
      "|     1|    293|   4.0|2005-04-02 23:31:43|\n",
      "|     1|    296|   4.0|2005-04-02 23:32:47|\n",
      "|     1|    318|   4.0|2005-04-02 23:33:18|\n",
      "|     1|    337|   3.5|2004-09-10 03:08:29|\n",
      "|     1|    367|   3.5|2005-04-02 23:53:00|\n",
      "|     1|    541|   4.0|2005-04-02 23:30:03|\n",
      "|     1|    589|   3.5|2005-04-02 23:45:57|\n",
      "|     1|    593|   3.5|2005-04-02 23:31:01|\n",
      "|     1|    653|   3.0|2004-09-10 03:08:11|\n",
      "|     1|    919|   3.5|2004-09-10 03:07:01|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "converting the timestamp formate to the year\n",
      "+------+-------+------+----+\n",
      "|userId|movieId|rating|year|\n",
      "+------+-------+------+----+\n",
      "|     1|      2|   3.5|2005|\n",
      "|     1|     29|   3.5|2005|\n",
      "|     1|     32|   3.5|2005|\n",
      "|     1|     47|   3.5|2005|\n",
      "|     1|     50|   3.5|2005|\n",
      "|     1|    112|   3.5|2004|\n",
      "|     1|    151|   4.0|2004|\n",
      "|     1|    223|   4.0|2005|\n",
      "|     1|    253|   4.0|2005|\n",
      "|     1|    260|   4.0|2005|\n",
      "|     1|    293|   4.0|2005|\n",
      "|     1|    296|   4.0|2005|\n",
      "|     1|    318|   4.0|2005|\n",
      "|     1|    337|   3.5|2004|\n",
      "|     1|    367|   3.5|2005|\n",
      "|     1|    541|   4.0|2005|\n",
      "|     1|    589|   3.5|2005|\n",
      "|     1|    593|   3.5|2005|\n",
      "|     1|    653|   3.0|2004|\n",
      "|     1|    919|   3.5|2004|\n",
      "+------+-------+------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "writing the files to the hdfs\n",
      "closing the application\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gcpPath = gs://scala-spark-temp/movies/\n",
       "ratingFile = rating.csv\n",
       "hdfsPath = hdfs://10.128.0.5:8020/user/suraj/movies/rating_years/\n",
       "ratingDf = [userId: int, movieId: int ... 2 more fields]\n",
       "ratingWithYearDf = [userId: int, movieId: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 2 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, from_unixtime, to_timestamp, year}\n",
    "\n",
    "val gcpPath = \"gs://scala-spark-temp/movies/\"\n",
    "val ratingFile = \"rating.csv\"\n",
    "val hdfsPath  = \"hdfs://10.128.0.5:8020/user/suraj/movies/rating_years/\"\n",
    "\n",
    "println(\"starting applications ...\")\n",
    "\n",
    "\n",
    "println(\"reading the rating csv from gcp\")\n",
    "val ratingDf = \n",
    "    spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(gcpPath + ratingFile)\n",
    "\n",
    "ratingDf.show(20)\n",
    "\n",
    "println(\"converting the timestamp formate to the year\")\n",
    "\n",
    "val ratingWithYearDf = \n",
    "    ratingDf.withColumn(\"year\", year(to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))).drop(\"timestamp\")\n",
    "\n",
    "ratingWithYearDf.show(20)\n",
    "\n",
    "println(\"writing the files to the hdfs\")\n",
    "ratingWithYearDf.write.partitionBy(\"year\").mode(\"overwrite\").parquet(hdfsPath + \"ratings.parquet\")\n",
    "\n",
    "println(\"closing the application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce8b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
